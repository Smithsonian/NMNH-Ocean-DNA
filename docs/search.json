[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Here you can find documentation on best practices, data management, utility scripts, and data analysis pipelines."
  },
  {
    "objectID": "qmd/utilityscripts/batchRenameFiles.html",
    "href": "qmd/utilityscripts/batchRenameFiles.html",
    "title": "Batch rename files",
    "section": "",
    "text": "Batch rename files\nScript name: batchRenameFiles.sh\nSource code\nScript to quickly rename many files. Can replace all (-m full) or part (-m partial) of the file names in a target directory.\nRequires the util-linux rename tool.\n\n\n\n\n\n\nWarning\n\n\n\nPLEASE consider making a backup of the target directory before running this script\n\n\nHelp documentation:\nSyntax: batchRenameFiles.sh [-h|d|t|m]\nOptions:\nh     Print this Help\nd     Directory containing files to rename\nt     Tab-delimited txt file with two columns:\n        First column: old file names or old strings in file names\n        Second column: new file names or new strings in file names\nm     File rename mode [full|partial]\n\nThis script requires UNIX line endings for the -t names table. If you created your names table using Excel or a Windows computer, it may have DOS line endings. You can easily convert your table to the correct format using dos2unix.\n\nFull replacement mode example:\n# download the script\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/utils/batchRenameFiles.sh\n# make a test directory\nmkdir full_test\n# add some empty test files\ntouch full_test/S1.txt full_test/S2.txt full_test/S3.txt\n# print contents of the test directory\nls -l full_test\n# create a tab-delimited table of old and new file names\necho -e \"S1.txt\\tsample1.text\" &gt; names.txt\necho -e \"S2.txt\\tsample2.text\" &gt;&gt; names.txt\necho -e \"S3.txt\\tsample3.text\" &gt;&gt; names.txt\n# check the table format\ncat names.txt\n# run the script\nbash batchRenameFiles.sh -m full -d ./full_test -t names.txt\n# print contents of the test directory\nls -l full_test\nPartial replacement mode example:\n\n\n\n\n\n\nWarning\n\n\n\nPartial replacement mode may fail if strings to be replaced are nested. For example, if you want to replace “s_1” with “s_A” and “s_10” with “s_B”, you will end up with “s_A” and “s_A0”, because “s_1” is nested in “s_10”.\n\n\n# download the script\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/utils/batchRenameFiles.sh\n# make a test directory\nmkdir partial_test\n# add some empty test files\ntouch partial_test/S1.txt partial_test/S2.txt partial_test/S3.txt partial_test/S1_S3.txt\n# print contents of the test directory\nls -l partial_test\n# create a tab-delimited table of old and new file names\necho -e \"S1\\tsample1\" &gt; names.txt\necho -e \"S2\\tsample2\" &gt;&gt; names.txt\necho -e \"S3\\tsample3\" &gt;&gt; names.txt\n# check the table format\ncat names.txt\n# run the script\nbash batchRenameFiles.sh -m partial -d ./partial_test -t names.txt\n# print contents of the test directory\nls -l partial_test"
  },
  {
    "objectID": "qmd/utilityscripts/recursiveReplaceStrings.html",
    "href": "qmd/utilityscripts/recursiveReplaceStrings.html",
    "title": "Recursive replace strings",
    "section": "",
    "text": "Recursive replace strings\nScript name: recursiveReplaceStrings.sh\nSource code\n\n\n\n\n\n\nWarning\n\n\n\nPLEASE consider making a backup of the target directory before running this script"
  },
  {
    "objectID": "qmd/hydrascripts/Go_fetch.html",
    "href": "qmd/hydrascripts/Go_fetch.html",
    "title": "go_fetch - download organelle or ribosomal sequences from GenBank",
    "section": "",
    "text": "go_fetch - download organelle or ribosomal sequences from GenBank\n\n\n\n\n\n\nNote\n\n\n\nAISO refers to a script format with 4 required sections: ABOUT, INPUTS, SCRIPT, and OUTPUTS. The user should only need to modify the INPUTS section of the script\n\n\nScript name: AISO_go_fetch.sh\nSource code\nThis script relies on go_fetch"
  },
  {
    "objectID": "qmd/bestpractices.html",
    "href": "qmd/bestpractices.html",
    "title": "Naming conventions",
    "section": "",
    "text": "FASTQ and FASTA files contain biological sequence data, usually nucleotide or amino acid sequences. FASTQ files contain sequence quality information (for example, Phred/Q scores), while FASTA files contain only the sequence.\nFor sequence data generated by the Ocean DNA genome skimming project, it is helpful to have a universal format for file names. This will facilitate downstream file parsing and data management.\nIdeas for required fields: - Voucher/Catalog ID - Taxonomic ID (Family-Genus-Species?), no abbreviations - anything else?\nNeed to consider the issue of when the same sample is sequenced twice. Include a unique identifier based on sequencing batch, plate, and well?\nNO SPACES in file name. Use consistent delimiter (underscore? period? dash?) to separate fields in the file name. Delimiter must NOT be used within the fields. If a field is unknown for a specific sample, insert NA, do not skip the field.\nExample: NMNH-12345_Percidae-Etheostoma-olmstedi_OtherStuff.fastq.gz\nWhy is this helpful? Let’s imagine you wanted to create a table to species names from a recent sequencing run. With a consistent naming scheme for your files, it’s easy!\n# print list of unique species in my FASTQ files\nfind /PATH/TO/MY/DIRECTORY/*.fastq.gz -printf \"%f\\n\" | cut -f2 -d\"_\" | cut -f2,3 -d \"-\" | sort | uniq\nPlease keep FASTQ/A files gzip compressed to minimize disk space.\nCompress a file: gzip fileName.fastq\nDecompress a file (if needed): gunzip fileName.fastq.gz",
    "crumbs": [
      "Best Practices",
      "Naming conventions"
    ]
  },
  {
    "objectID": "qmd/bestpractices.html#fastqa-files---work-in-progress",
    "href": "qmd/bestpractices.html#fastqa-files---work-in-progress",
    "title": "Naming conventions",
    "section": "",
    "text": "FASTQ and FASTA files contain biological sequence data, usually nucleotide or amino acid sequences. FASTQ files contain sequence quality information (for example, Phred/Q scores), while FASTA files contain only the sequence.\nFor sequence data generated by the Ocean DNA genome skimming project, it is helpful to have a universal format for file names. This will facilitate downstream file parsing and data management.\nIdeas for required fields: - Voucher/Catalog ID - Taxonomic ID (Family-Genus-Species?), no abbreviations - anything else?\nNeed to consider the issue of when the same sample is sequenced twice. Include a unique identifier based on sequencing batch, plate, and well?\nNO SPACES in file name. Use consistent delimiter (underscore? period? dash?) to separate fields in the file name. Delimiter must NOT be used within the fields. If a field is unknown for a specific sample, insert NA, do not skip the field.\nExample: NMNH-12345_Percidae-Etheostoma-olmstedi_OtherStuff.fastq.gz\nWhy is this helpful? Let’s imagine you wanted to create a table to species names from a recent sequencing run. With a consistent naming scheme for your files, it’s easy!\n# print list of unique species in my FASTQ files\nfind /PATH/TO/MY/DIRECTORY/*.fastq.gz -printf \"%f\\n\" | cut -f2 -d\"_\" | cut -f2,3 -d \"-\" | sort | uniq\nPlease keep FASTQ/A files gzip compressed to minimize disk space.\nCompress a file: gzip fileName.fastq\nDecompress a file (if needed): gunzip fileName.fastq.gz",
    "crumbs": [
      "Best Practices",
      "Naming conventions"
    ]
  },
  {
    "objectID": "qmd/dataManagementScripts/DNA_QC.html",
    "href": "qmd/dataManagementScripts/DNA_QC.html",
    "title": "DNA sequence data quality control",
    "section": "",
    "text": "Script name: run_DNA_QC.sh\nSource code\nScript to generate QC reports for many DNA FASTQ files. Useful to check the quality of a sequencing run prior to downstream analyses.\nThis script was written specifically for the Smithsonian Hydra cluster, but could be modified to work for other computing environments. It uses FastQC and MultiQC to generate reports. It also performs default fastp filtering and generates post-filtering reports.\n\n\nFirst, you need to download and install Nextflow. On Hydra, run the following commands.\n# Nextflow installation instructions from https://www.nextflow.io/docs/latest/install.html\ncd ~\nmodule load tools/java/21.0.2\ncurl -s https://get.nextflow.io | bash # install Nextflow\nchmod +x nextflow # make Nextflow executable\nmkdir ~/bin # create bin directory, if needed\nmv ~/nextflow ~/bin/nextflow # move nextflow to bin directory\necho 'export PATH=\"${HOME}/bin:${PATH}\"' &gt;&gt; ~/.bashrc # add bin directory to PATH, in case it's not already there\nsource ~/.bashrc\nThis will allow you to run nextflow from anywhere on the cluster (if you have the java 21.0.2 module loaded).\nNext, we need to download the Nextflow workflow and config.\nmkdir ~/DNA_QC_scripts\ncd ~/DNA_QC_scripts\n# download the Nextflow workflow\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/DNA_QC/DNA_QC.nf\n# download the Nextflow config file\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/DNA_QC/Hydra.nf.config\n\n\n\nDecide where you want to store the QC reports and navigate there on the command line. Then download the run_DNA_QC.sh script. For example:\nmkdir ~/my_project_QC # create a new directory in your home directory\ncd ~/my_project_QC \nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/DNA_QC/run_DNA_QC.sh\nNow open run_DNA_QC.sh and modify the RAW_DATA_DIR and READS_SUFFIX variables.\n\nRAW_DATA_DIR: Path to a directory containing the sequence data. All files should all be FASTQ and gzipped (i.e. end in “.fastq.gz”).\nREADS_SUFFIX: The file name suffix linking the forward and reverse reads. For example, if “sampleA” has two paired read files “sampleA_R1.fastq.gz” and “sampleA_R2.fastq.gz”, the suffix would be “_R{1,2}.fastq.gz”.\n\nFinally, run the script.\nbash run_DNA_QC.sh\nOutput will be written to QC_results. To see a summary of all samples, download the HTML files in QC_results/multiqc and open with a web browser.\nIf you have many samples, consider running run_DNA_QC.sh as a batch job with qsub. You will need to use the special workflow manager queue “lTWFM.sq” in order to run Nextflow inside of a batch job.\n\n\n\nYou can change the filtering settings by modifying the shell section of the FASTP process block in the DNA_QC.nf workflow. For example, if you want to trim all reads to a maximum length of 100 bp:\n    shell:\n    '''\n        fastp -i !{reads[0]} \\\n            -I !{reads[1]} \\\n            -o !{reads[0].simpleName}_trimmed.fastq.gz \\\n            -O !{reads[1].simpleName}_trimmed.fastq.gz \\\n            --max_len 100\n    '''\nSee the fastp CLI reference for a complete list of options.",
    "crumbs": [
      "Data Management Scripts",
      "DNA sequence data quality control"
    ]
  },
  {
    "objectID": "qmd/dataManagementScripts/DNA_QC.html#setup",
    "href": "qmd/dataManagementScripts/DNA_QC.html#setup",
    "title": "DNA sequence data quality control",
    "section": "",
    "text": "First, you need to download and install Nextflow. On Hydra, run the following commands.\n# Nextflow installation instructions from https://www.nextflow.io/docs/latest/install.html\ncd ~\nmodule load tools/java/21.0.2\ncurl -s https://get.nextflow.io | bash # install Nextflow\nchmod +x nextflow # make Nextflow executable\nmkdir ~/bin # create bin directory, if needed\nmv ~/nextflow ~/bin/nextflow # move nextflow to bin directory\necho 'export PATH=\"${HOME}/bin:${PATH}\"' &gt;&gt; ~/.bashrc # add bin directory to PATH, in case it's not already there\nsource ~/.bashrc\nThis will allow you to run nextflow from anywhere on the cluster (if you have the java 21.0.2 module loaded).\nNext, we need to download the Nextflow workflow and config.\nmkdir ~/DNA_QC_scripts\ncd ~/DNA_QC_scripts\n# download the Nextflow workflow\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/DNA_QC/DNA_QC.nf\n# download the Nextflow config file\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/DNA_QC/Hydra.nf.config",
    "crumbs": [
      "Data Management Scripts",
      "DNA sequence data quality control"
    ]
  },
  {
    "objectID": "qmd/dataManagementScripts/DNA_QC.html#running-the-script",
    "href": "qmd/dataManagementScripts/DNA_QC.html#running-the-script",
    "title": "DNA sequence data quality control",
    "section": "",
    "text": "Decide where you want to store the QC reports and navigate there on the command line. Then download the run_DNA_QC.sh script. For example:\nmkdir ~/my_project_QC # create a new directory in your home directory\ncd ~/my_project_QC \nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/DNA_QC/run_DNA_QC.sh\nNow open run_DNA_QC.sh and modify the RAW_DATA_DIR and READS_SUFFIX variables.\n\nRAW_DATA_DIR: Path to a directory containing the sequence data. All files should all be FASTQ and gzipped (i.e. end in “.fastq.gz”).\nREADS_SUFFIX: The file name suffix linking the forward and reverse reads. For example, if “sampleA” has two paired read files “sampleA_R1.fastq.gz” and “sampleA_R2.fastq.gz”, the suffix would be “_R{1,2}.fastq.gz”.\n\nFinally, run the script.\nbash run_DNA_QC.sh\nOutput will be written to QC_results. To see a summary of all samples, download the HTML files in QC_results/multiqc and open with a web browser.\nIf you have many samples, consider running run_DNA_QC.sh as a batch job with qsub. You will need to use the special workflow manager queue “lTWFM.sq” in order to run Nextflow inside of a batch job.",
    "crumbs": [
      "Data Management Scripts",
      "DNA sequence data quality control"
    ]
  },
  {
    "objectID": "qmd/dataManagementScripts/DNA_QC.html#optional-adjust-filtering",
    "href": "qmd/dataManagementScripts/DNA_QC.html#optional-adjust-filtering",
    "title": "DNA sequence data quality control",
    "section": "",
    "text": "You can change the filtering settings by modifying the shell section of the FASTP process block in the DNA_QC.nf workflow. For example, if you want to trim all reads to a maximum length of 100 bp:\n    shell:\n    '''\n        fastp -i !{reads[0]} \\\n            -I !{reads[1]} \\\n            -o !{reads[0].simpleName}_trimmed.fastq.gz \\\n            -O !{reads[1].simpleName}_trimmed.fastq.gz \\\n            --max_len 100\n    '''\nSee the fastp CLI reference for a complete list of options.",
    "crumbs": [
      "Data Management Scripts",
      "DNA sequence data quality control"
    ]
  },
  {
    "objectID": "qmd/dataManagementScripts/raw_sequence_validate.html",
    "href": "qmd/dataManagementScripts/raw_sequence_validate.html",
    "title": "Validate Hydra Store raw sequence data and metadata",
    "section": "",
    "text": "Validate Hydra Store raw sequence data and metadata\nScript name: validate_seq_data.py or validate_seq_data.sh\npython source code\nbash source code\nValidates Ocean DNA raw sequence data against its corresponding metadata. The python script is faster than the bash script.\nTo download the script:\n# download python script\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/raw_sequence_validate/validate_seq_data.py\n# download bash script\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/data_management/raw_sequence_validate/validate_seq_data.sh\nRun python validate_seq_data.py -h or bash validate_seq_data.sh -h to print the help documentation.\nDESCRIPTION:\n    Validates Ocean DNA raw sequence data against its corresponding metadata.\n    This script checks for correct naming, file existence, and ensures that\n    the contents of the metadata file and sequence data directory match.\n\nARGUMENTS:\n  MAP_FILE\n      Path to a text file mapping metadata to sequence data directories.\n      It requires a header row (which is ignored). Each subsequent line\n      should contain two space-separated columns:\n\n      Column 1: The metadata CSV filename.\n                - Must be in \"/store/public/oceandna/raw_sequence_metadata\"\n                - Must end with \"_mapfile.csv\"\n                - Its header must start with \"ID,R1,R2,Taxon,UniqueID\"\n\n      Column 2: The raw sequence data directory name.\n                - Must be in \"/store/public/oceandna/raw_sequence_data\"\n                - Must contain one or more .fastq.gz files\n\nOPTIONS:\n  -h, --help\n      Display this help message and exit.\nExample MAP_FILE.",
    "crumbs": [
      "Data Management Scripts",
      "Validate Hydra Store raw sequence data and metadata"
    ]
  },
  {
    "objectID": "qmd/datamanagement.html",
    "href": "qmd/datamanagement.html",
    "title": "Data management guide",
    "section": "",
    "text": "The Ocean DNA group has access to the following disk storage systems:\n\nHydra cluster scratch\n\n/scratch/nmnh_ocean_dna\n40 TB\nNot backed up by Hydra admin\nNo automatic file purging\n\nHydra cluster store\n\n/store/nmnh_ocean_dna\n40 TB\nNot backed up by Hydra admin\nNo automatic file purging\nIntended for large raw data files and inactive projects\nSlower read/write speeds\nCan’t be used for active analysis\n\nSmithsonian network P drive\n\nsmb://si-ocio-qnas2.si.edu/nmnh/nmnh-all/public/nmnh-ocean-dna\n80 TB\nIncrementally backed up daily\nFully backed up weekly\nOnly accessible from SI computers\n\n\nThe Hydra spaces\n\n\n\nThe following is a proposed data management guide for SI Ocean DNA sequence data. This workflow was designed for genome skimming datasets, but could be adapted for other project types.\n\n\n\n\n\ngraph TD;\n\n  GenoHub[**GenoHub**]\n  Metadata[**Map File**]\n  Analyses(Run quality/adapter trimming, mitogenome assembly, etc)\n  Scratch[(**Hydra Scratch**)]\n  Store[(**Hydra Store**)]\n  PDrive[(**P Drive**)]\n  \n  Move0[STEP 2: create map file]\n  Move1[STEP 1: download FASTQs]  \n  Move3[copy important results]\n  Move4[STEP 3: move to Hydra Store]\n  Move5[STEP 4: backup to P drive]\n\n  Move0--&gt;Metadata\n  Metadata--&gt;Move4\n  GenoHub--&gt;Move1\n  GenoHub--&gt;Move0\n  Scratch--&gt;Move4\n  Move1--&gt;Scratch\n  subgraph \" \"\n    Scratch--&gt;Analyses\n    Analyses--&gt;Move3\n    Move3--&gt;Store\n    Move4--&gt;Store\n  end\n  Store--&gt;Move5\n  Move5--&gt;PDrive\n\n  classDef process stroke:black,color:white,fill:#159BD7,stroke-dasharray: 5 5\n  classDef storage stroke:black,color:white,fill:#159BD7\n  classDef ccr stroke:black,color:white,fill:#159BD7\n  classDef step stroke:black,color:black,fill:#a0c5fa,stroke-dasharray: 5 5\n\n  class Rename,Analyses,Move2,Move3 process\n  class Metadata,GenoHub,Scratch,Store,PDrive storage\n  class Move0,Move1,Move4,Move5 step\n\n  click Rename \"bestpractices.html\"\n\n  linkStyle default stroke:grey, stroke-width:4px\n\n\n\n\n\n\n\n\n\n\nBelow are step-by-step instructions for our data management workflow.\n\n\nGenoHub will provide a link with instructions to download the raw sequence data. The data should be demultiplexed and compressed sequence reads in FASTQ format (i.e. files should end in “.fastq.gz” or “.fq.gz”).\nYou can download sequence data directly from GenoHub to Hydra using the AWS command line interface. First, load the AWS module and run configuration. Enter the information provided on the GenoHub download instructions page when prompted.\nmodule load tools/awscli/2.15.27\naws configure\nNow you can access the AWS storage bucket. To list the contents prior to downloading, run the following, replacing “XXXXXX” with your GenoHub project number.\naws s3 ls s3://genohubXXXXXX\nRun the following command to download your data. Replace “XXXXXX” with your GenoHub project number and specify the path to your desired download location.\n\n\n\n\n\n\nNote\n\n\n\nWe recommend downloading sequence data to /scratch/public/genomics/YOUR_USER_ID. This directory has the fastest read/write speeds and the largest storage quota.\n\n\naws s3 sync s3://genohubXXXXXXX /full/path/to/you/download/directory\n\n\n\nThe map file is a CSV (comma-separated file) containing information for all samples in the GenoHub project. This file is crucial to ensure that your data are not only backed up, but usable.\nPlease use the following naming convention for your map file:\ngenohub-{GENOHUB_PROJECT_NUMBER}_{PROJECT_DESCRIPTION}_mapfile.csv\nFor example: genohub-8459898_Vietnam_mapfile.csv\nWhere “8459898” is the GenoHub project number and “Vietnam” is the description given by the project owner.\n\n\n\n\n\n\nWarning\n\n\n\nDo not include underscores or spaces in the project description.\n\n\nThe first five columns of your map file must be:\n\nID: GenoHub sample name. For example:\n\nI-0025-99AD-1N1\n\nR1: Read 1 FASTQ file name. For example:\n\nI-0025-99AD-1N1_1.fastq.gz\n\nR2: Read 2 FASTQ file name. For example:\n\nI-0025-99AD-1N1_2.fastq.gz\n\nTaxon: Your best guess at taxonomic assignment, no required format. For example:\n\nLatin binomial: Urophycis chuss\nGenus: Urophycis sp.\nHigher level taxonomy: Phycidae\nCombination: Gadiformes_Phycidae_Urophycis_chuss\n\nUniqID: Identifier linked to a voucher/tissue sample, no required format. For example:\n\nUSNM catalog number: USNM 477715\nBiorepository EZID: http://n2t.net/ark:/65665/3987722bd-99bc-4913-a279-092f58c82d72\nGEOME BCID: https://n2t.net/ark:/21547/FxY2USNM_Fish_477715.1\n\n\nYou may include other metadata as additional columns in the map file.\n\n\n\n\n\n\nNote\n\n\n\nWe strongly advise the use of globally/universally unique identifiers (GUIDs/UUIDs) for the UniqID column. If you need to generate these IDs, consider creating a project in GEOME.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe first five columns must contain information for all samples in the map file. We will contact you if there is any missing data.\n\n\n\n\n\nThe Ocean DNA Store directory on Hydra contains two subdirectories where you should upload your data.\n\n\nEach directory within raw_sequence_data should contain the results of a GenoHub sequencing project. To save disk space, please ensure that all sequence files are compressed (e.g. sequence1.fastq.gz).\nPlease use the following naming convention for your directories:\ngenohub-{GENOHUB_PROJECT_NUMBER}_{PROJECT_DESCRIPTION}\nFor example: “genohub-8459898_Vietnam”\nWhere “8459898” is the GenoHub project number and “Vietnam” is the description given by the project owner.\n\n\n\n\n\n\nWarning\n\n\n\nDo not include underscores or spaces in the project description.\n\n\nTo create a new directory for your project:\ncd /store/nmnh_ocean_dna/raw_sequence_data\nmkdir genohub-XXXXXX_MY-PROJECT-DESCRIPTION\nYou can now copy your sequence data to this directory. There are many options, including:\n\nThe Linux cp command (not recommended)\n\ncopies one file at a time, fairly slow\ndoes not check for complete file transfer, which can lead to corrupt files at the destination\n\nrclone\n\navailable on Hydra with module load XXXXXX\ncopies multiple files in parallel\nautomatically performs checks to ensure complete file transfer\ncan resume an interrupted transfer by running the same rclone command again\nexample usage:\n\nperform a “dry run” (list files but do not copy):\n\nrclone copy -v -n /PATH/TO/SOURCE /PATH/TO/DESTINATION\n\ncopy files:\n\nrclone copy -v /PATH/TO/SOURCE /PATH/TO/DESTINATION\n\ncopy only files that end in “.fastq.gz”\n\nrclone copy -v --include \"*.fastq.gz\" /PATH/TO/SOURCE /PATH/TO/DESTINATION\n\nGlobus\n\nweb-based file transfer service\nnormally used to transfer data between servers, but can also perform data transfers within Hydra\ndoes not require you to maintain an active connection to Hydra while files transfer\nsee the documentation for additional information\n\n\n\n\n\nEach directory in raw_sequence_data must be accompanied by a CSV map file in raw_sequence_metadata. The names of the map files should match the names of the sequence directories (with the addition of _mapfile.csv).\nYou can upload the map file to raw_sequence_metadata using your favorite SCP or SFTP client (e.g. FileZilla, WinSCP, Cyberduck).\n\n\n\n\nAfter adding new sequence data and the corresponding map file to Hydra Store, please contact Dan MacGuigan. He will copy your data to the Ocean DNA P drive directory. Data transfer may take several hours, depending on the size of your dataset.\nThe P drive is a separate system from Hydra and is backed up frequently by the Smithsonian OCIO. To keep these data backups secure, Dan MacGuigan is the only person with access to the Ocean DNA P drive.",
    "crumbs": [
      "Data Management",
      "Data management guide"
    ]
  },
  {
    "objectID": "qmd/datamanagement.html#ocean-dna-data-storage-and-compute-resources",
    "href": "qmd/datamanagement.html#ocean-dna-data-storage-and-compute-resources",
    "title": "Data management guide",
    "section": "",
    "text": "The Ocean DNA group has access to the following disk storage systems:\n\nHydra cluster scratch\n\n/scratch/nmnh_ocean_dna\n40 TB\nNot backed up by Hydra admin\nNo automatic file purging\n\nHydra cluster store\n\n/store/nmnh_ocean_dna\n40 TB\nNot backed up by Hydra admin\nNo automatic file purging\nIntended for large raw data files and inactive projects\nSlower read/write speeds\nCan’t be used for active analysis\n\nSmithsonian network P drive\n\nsmb://si-ocio-qnas2.si.edu/nmnh/nmnh-all/public/nmnh-ocean-dna\n80 TB\nIncrementally backed up daily\nFully backed up weekly\nOnly accessible from SI computers\n\n\nThe Hydra spaces",
    "crumbs": [
      "Data Management",
      "Data management guide"
    ]
  },
  {
    "objectID": "qmd/datamanagement.html#data-management-flowchart",
    "href": "qmd/datamanagement.html#data-management-flowchart",
    "title": "Data management guide",
    "section": "",
    "text": "The following is a proposed data management guide for SI Ocean DNA sequence data. This workflow was designed for genome skimming datasets, but could be adapted for other project types.\n\n\n\n\n\ngraph TD;\n\n  GenoHub[**GenoHub**]\n  Metadata[**Map File**]\n  Analyses(Run quality/adapter trimming, mitogenome assembly, etc)\n  Scratch[(**Hydra Scratch**)]\n  Store[(**Hydra Store**)]\n  PDrive[(**P Drive**)]\n  \n  Move0[STEP 2: create map file]\n  Move1[STEP 1: download FASTQs]  \n  Move3[copy important results]\n  Move4[STEP 3: move to Hydra Store]\n  Move5[STEP 4: backup to P drive]\n\n  Move0--&gt;Metadata\n  Metadata--&gt;Move4\n  GenoHub--&gt;Move1\n  GenoHub--&gt;Move0\n  Scratch--&gt;Move4\n  Move1--&gt;Scratch\n  subgraph \" \"\n    Scratch--&gt;Analyses\n    Analyses--&gt;Move3\n    Move3--&gt;Store\n    Move4--&gt;Store\n  end\n  Store--&gt;Move5\n  Move5--&gt;PDrive\n\n  classDef process stroke:black,color:white,fill:#159BD7,stroke-dasharray: 5 5\n  classDef storage stroke:black,color:white,fill:#159BD7\n  classDef ccr stroke:black,color:white,fill:#159BD7\n  classDef step stroke:black,color:black,fill:#a0c5fa,stroke-dasharray: 5 5\n\n  class Rename,Analyses,Move2,Move3 process\n  class Metadata,GenoHub,Scratch,Store,PDrive storage\n  class Move0,Move1,Move4,Move5 step\n\n  click Rename \"bestpractices.html\"\n\n  linkStyle default stroke:grey, stroke-width:4px",
    "crumbs": [
      "Data Management",
      "Data management guide"
    ]
  },
  {
    "objectID": "qmd/datamanagement.html#step-by-step-data-management-workflow",
    "href": "qmd/datamanagement.html#step-by-step-data-management-workflow",
    "title": "Data management guide",
    "section": "",
    "text": "Below are step-by-step instructions for our data management workflow.\n\n\nGenoHub will provide a link with instructions to download the raw sequence data. The data should be demultiplexed and compressed sequence reads in FASTQ format (i.e. files should end in “.fastq.gz” or “.fq.gz”).\nYou can download sequence data directly from GenoHub to Hydra using the AWS command line interface. First, load the AWS module and run configuration. Enter the information provided on the GenoHub download instructions page when prompted.\nmodule load tools/awscli/2.15.27\naws configure\nNow you can access the AWS storage bucket. To list the contents prior to downloading, run the following, replacing “XXXXXX” with your GenoHub project number.\naws s3 ls s3://genohubXXXXXX\nRun the following command to download your data. Replace “XXXXXX” with your GenoHub project number and specify the path to your desired download location.\n\n\n\n\n\n\nNote\n\n\n\nWe recommend downloading sequence data to /scratch/public/genomics/YOUR_USER_ID. This directory has the fastest read/write speeds and the largest storage quota.\n\n\naws s3 sync s3://genohubXXXXXXX /full/path/to/you/download/directory\n\n\n\nThe map file is a CSV (comma-separated file) containing information for all samples in the GenoHub project. This file is crucial to ensure that your data are not only backed up, but usable.\nPlease use the following naming convention for your map file:\ngenohub-{GENOHUB_PROJECT_NUMBER}_{PROJECT_DESCRIPTION}_mapfile.csv\nFor example: genohub-8459898_Vietnam_mapfile.csv\nWhere “8459898” is the GenoHub project number and “Vietnam” is the description given by the project owner.\n\n\n\n\n\n\nWarning\n\n\n\nDo not include underscores or spaces in the project description.\n\n\nThe first five columns of your map file must be:\n\nID: GenoHub sample name. For example:\n\nI-0025-99AD-1N1\n\nR1: Read 1 FASTQ file name. For example:\n\nI-0025-99AD-1N1_1.fastq.gz\n\nR2: Read 2 FASTQ file name. For example:\n\nI-0025-99AD-1N1_2.fastq.gz\n\nTaxon: Your best guess at taxonomic assignment, no required format. For example:\n\nLatin binomial: Urophycis chuss\nGenus: Urophycis sp.\nHigher level taxonomy: Phycidae\nCombination: Gadiformes_Phycidae_Urophycis_chuss\n\nUniqID: Identifier linked to a voucher/tissue sample, no required format. For example:\n\nUSNM catalog number: USNM 477715\nBiorepository EZID: http://n2t.net/ark:/65665/3987722bd-99bc-4913-a279-092f58c82d72\nGEOME BCID: https://n2t.net/ark:/21547/FxY2USNM_Fish_477715.1\n\n\nYou may include other metadata as additional columns in the map file.\n\n\n\n\n\n\nNote\n\n\n\nWe strongly advise the use of globally/universally unique identifiers (GUIDs/UUIDs) for the UniqID column. If you need to generate these IDs, consider creating a project in GEOME.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe first five columns must contain information for all samples in the map file. We will contact you if there is any missing data.\n\n\n\n\n\nThe Ocean DNA Store directory on Hydra contains two subdirectories where you should upload your data.\n\n\nEach directory within raw_sequence_data should contain the results of a GenoHub sequencing project. To save disk space, please ensure that all sequence files are compressed (e.g. sequence1.fastq.gz).\nPlease use the following naming convention for your directories:\ngenohub-{GENOHUB_PROJECT_NUMBER}_{PROJECT_DESCRIPTION}\nFor example: “genohub-8459898_Vietnam”\nWhere “8459898” is the GenoHub project number and “Vietnam” is the description given by the project owner.\n\n\n\n\n\n\nWarning\n\n\n\nDo not include underscores or spaces in the project description.\n\n\nTo create a new directory for your project:\ncd /store/nmnh_ocean_dna/raw_sequence_data\nmkdir genohub-XXXXXX_MY-PROJECT-DESCRIPTION\nYou can now copy your sequence data to this directory. There are many options, including:\n\nThe Linux cp command (not recommended)\n\ncopies one file at a time, fairly slow\ndoes not check for complete file transfer, which can lead to corrupt files at the destination\n\nrclone\n\navailable on Hydra with module load XXXXXX\ncopies multiple files in parallel\nautomatically performs checks to ensure complete file transfer\ncan resume an interrupted transfer by running the same rclone command again\nexample usage:\n\nperform a “dry run” (list files but do not copy):\n\nrclone copy -v -n /PATH/TO/SOURCE /PATH/TO/DESTINATION\n\ncopy files:\n\nrclone copy -v /PATH/TO/SOURCE /PATH/TO/DESTINATION\n\ncopy only files that end in “.fastq.gz”\n\nrclone copy -v --include \"*.fastq.gz\" /PATH/TO/SOURCE /PATH/TO/DESTINATION\n\nGlobus\n\nweb-based file transfer service\nnormally used to transfer data between servers, but can also perform data transfers within Hydra\ndoes not require you to maintain an active connection to Hydra while files transfer\nsee the documentation for additional information\n\n\n\n\n\nEach directory in raw_sequence_data must be accompanied by a CSV map file in raw_sequence_metadata. The names of the map files should match the names of the sequence directories (with the addition of _mapfile.csv).\nYou can upload the map file to raw_sequence_metadata using your favorite SCP or SFTP client (e.g. FileZilla, WinSCP, Cyberduck).\n\n\n\n\nAfter adding new sequence data and the corresponding map file to Hydra Store, please contact Dan MacGuigan. He will copy your data to the Ocean DNA P drive directory. Data transfer may take several hours, depending on the size of your dataset.\nThe P drive is a separate system from Hydra and is backed up frequently by the Smithsonian OCIO. To keep these data backups secure, Dan MacGuigan is the only person with access to the Ocean DNA P drive.",
    "crumbs": [
      "Data Management",
      "Data management guide"
    ]
  },
  {
    "objectID": "qmd/hydrascripts/BLAST_18s_28s.html",
    "href": "qmd/hydrascripts/BLAST_18s_28s.html",
    "title": "BLAST - find 18s & 28s in genome assembly",
    "section": "",
    "text": "Script name: BLAST_18s-28s_hydra.sh\nsource code\nThis script is meant to help identify and extract contigs/scaffolds containing the 18s and 28s genes.\nTo download the script:\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/Hydra/BLAST_Hydra/BLAST_18s-28s_hydra.sh\nTry running bash batchRenameFiles.sh -h to print the help documentation.\nScript to find and rename contigs/scaffolds containing\n18s and/or 28s genes\n\nauthor: Dan MacGuigan\ncontact: macguigand@si.edu\n\nOptions:\nc   FASTA file containing genomics scaffolds or contigs\ni   sample ID, will be used to name resulting files and BLAST hits\ns   FASTA file containing query 18s sequence\nl   FASTA file containing query 28s sequence\nh   Print this Help\n\nUsage:\nbash BLAST_18s-28s_hydra.sh -c my_contigs.fasta -i my_sample_ID -s my_18s.fasta -l my_28s.fasta\n\n\n\nScript name: BLAST_job.sh\nsource code\nThis script is a wrapper for BLAST_18s-28s_hydra.sh, allowing you analyze multiple genome assemblies.\nTo download the script:\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/Hydra/BLAST_Hydra/BLAST_job.sh\nYou will then need to modify the INPUTS section.\n# INPUTS ################################################\n\n# working directory containing contig/scaffold FASTA files\nDIR=\"/pool/public/genomics/macguigand/BLAST_testing/scaffolds\"\n\n# FASTA file suffix (e.g. \"fasta\", \"fa\", \"fas\")\n# must be the same for all files in DIR\nSUFFIX=\"fasta\"\n\n# full path to 18s and 28s query sequences\nrRNA_S=\"/pool/public/genomics/macguigand/BLAST_testing/18s.fasta\"\nrRNA_L=\"/pool/public/genomics/macguigand/BLAST_testing/28s.fasta\"\n\n# full path to your copy of the BLAST_18s-28s_hydra.sh script\nBLAST_SCRIPT=\"/pool/public/genomics/macguigand/BLAST_testing/BLAST_18s-28s_hydra.sh\"\nRunning qsub BLAST_job.sh will submit the script to the cluster’s job scheduler.\nOnce complete, BLAST hits will be written to a new folder BLAST_hits within DIR."
  },
  {
    "objectID": "qmd/hydrascripts/BLAST_18s_28s.html#blast_18s-28s_hydra.sh",
    "href": "qmd/hydrascripts/BLAST_18s_28s.html#blast_18s-28s_hydra.sh",
    "title": "BLAST - find 18s & 28s in genome assembly",
    "section": "",
    "text": "Script name: BLAST_18s-28s_hydra.sh\nsource code\nThis script is meant to help identify and extract contigs/scaffolds containing the 18s and 28s genes.\nTo download the script:\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/Hydra/BLAST_Hydra/BLAST_18s-28s_hydra.sh\nTry running bash batchRenameFiles.sh -h to print the help documentation.\nScript to find and rename contigs/scaffolds containing\n18s and/or 28s genes\n\nauthor: Dan MacGuigan\ncontact: macguigand@si.edu\n\nOptions:\nc   FASTA file containing genomics scaffolds or contigs\ni   sample ID, will be used to name resulting files and BLAST hits\ns   FASTA file containing query 18s sequence\nl   FASTA file containing query 28s sequence\nh   Print this Help\n\nUsage:\nbash BLAST_18s-28s_hydra.sh -c my_contigs.fasta -i my_sample_ID -s my_18s.fasta -l my_28s.fasta"
  },
  {
    "objectID": "qmd/hydrascripts/BLAST_18s_28s.html#blast_job.sh",
    "href": "qmd/hydrascripts/BLAST_18s_28s.html#blast_job.sh",
    "title": "BLAST - find 18s & 28s in genome assembly",
    "section": "",
    "text": "Script name: BLAST_job.sh\nsource code\nThis script is a wrapper for BLAST_18s-28s_hydra.sh, allowing you analyze multiple genome assemblies.\nTo download the script:\nwget https://raw.githubusercontent.com/Smithsonian/NMNH-Ocean-DNA/refs/heads/main/scripts/Hydra/BLAST_Hydra/BLAST_job.sh\nYou will then need to modify the INPUTS section.\n# INPUTS ################################################\n\n# working directory containing contig/scaffold FASTA files\nDIR=\"/pool/public/genomics/macguigand/BLAST_testing/scaffolds\"\n\n# FASTA file suffix (e.g. \"fasta\", \"fa\", \"fas\")\n# must be the same for all files in DIR\nSUFFIX=\"fasta\"\n\n# full path to 18s and 28s query sequences\nrRNA_S=\"/pool/public/genomics/macguigand/BLAST_testing/18s.fasta\"\nrRNA_L=\"/pool/public/genomics/macguigand/BLAST_testing/28s.fasta\"\n\n# full path to your copy of the BLAST_18s-28s_hydra.sh script\nBLAST_SCRIPT=\"/pool/public/genomics/macguigand/BLAST_testing/BLAST_18s-28s_hydra.sh\"\nRunning qsub BLAST_job.sh will submit the script to the cluster’s job scheduler.\nOnce complete, BLAST hits will be written to a new folder BLAST_hits within DIR."
  },
  {
    "objectID": "qmd/hydrascripts/GetOrganelle.html",
    "href": "qmd/hydrascripts/GetOrganelle.html",
    "title": "GetOrganelle - targeted organelle assembly",
    "section": "",
    "text": "GetOrganelle - targeted organelle assembly\n\n\n\n\n\n\nNote\n\n\n\nAISO refers to a script format with 4 required sections: ABOUT, INPUTS, SCRIPT, and OUTPUTS. The user should only need to modify the INPUTS section of the script\n\n\nScript name: AISO_GetOrganelle_Hydra.sh\nSource code"
  },
  {
    "objectID": "qmd/utilityscripts/validate_biosample_taxonomy.html",
    "href": "qmd/utilityscripts/validate_biosample_taxonomy.html",
    "title": "Validate BioProject taxonomy",
    "section": "",
    "text": "Validate BioProject taxonomy\nScript name: validate_biosample_taxonomy.py\nSource code\nThis python script checks the taxonomy between all GenBank nucleotide records and associated BioSamples in a specific BioProject. It verifies that the ORGANISM field of the GenBank record matches the BioSample OrganismName. Mismatches are written to a CSV file, taxon_mismatches.csv.\nIf a GenBank nucleotide record does not have an associated BioSample, it the GenBank accession number is written to the file GenBank_records_no_BioSample.txt.\nThis script has the following dependencies:\n\npython 3\nbiopython\n\nThis could be easily set up in a conda environment. For example:\nconda create --name biopython conda-forge::biopython\nThis script takes 3 arguments\n\nA NCBI BioProject number (e.g. PRJNA720393)\nYour email address, required for Entrez tools\nYour NCBI API key (instructions to generate a key)\n\nExample usage:\npython validate_biosample_taxonomy.py \"PRJNXXXXX\" \"YOUR_EMAIL\" \"YOUR_NCBI_API_KEY\"\n\n\n\n\n\n\nWarning\n\n\n\nNCBI Entrez Programming Utilities (E-utilities) are limited to 10,000 queries at a time (retmax = 10000). If your BioProject contains more than 10,000 GenBank records, this script will not work.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis script may randomly crash. This is usually due to a http request timeout. Try restarting the script if this happens."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SI Ocean DNA",
    "section": "",
    "text": "Welcome to the SI Ocean DNA landing page."
  }
]